{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Introduction to Agents with LangGraph\n",
    "\n",
    "In this notebook you'll learn how to create Agents using LangGraph.\n",
    "\n",
    "We will use an LLM to answer questions, and execute actions for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Run the cell below to import a couple of basic libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from pprint import pprint\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this challenge you'll need your API key again.\n",
    "\n",
    "üëâ Run the cell below to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Why Agents?\n",
    "\n",
    "Remember how we used an LLM with tool calling (aka function calling)?\n",
    "\n",
    "First, we gave the LLM information about the tools / functions we have. Then, we asked the LLM natural language questions and it told us which tool it would use, and it gave us the arguments we'd need to use to cool the tool.\n",
    "\n",
    "That was already pretty cool, but it left the actual calling of the tool up to us.\n",
    "\n",
    "With agents, we'll have the LLM decide which tool to use and with arguments, and the agent will execute the work using the tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü¶æ Context: our own PA\n",
    "\n",
    "In this challenge, we'll create our own PA, our Personal Agent.\n",
    "\n",
    "The agent will be able to help us with a couple of our routine daily tasks:\n",
    "\n",
    "- Update us on our stock portfolio.\n",
    "- Get recipes (can't live of stock prices, you got to eat too).\n",
    "- Retrieve information from Wikipediat (to feed your brain)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many frameworks to create agents.\n",
    "\n",
    "In this notebook we'll use LangGraph, from the creators of LangChain ü¶úüîó. It will interact nicely with the tools we've already seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Setup our brain\n",
    "\n",
    "In it's most simple form, an agent consist of a brain (an LLM), an agent executor, and different tools.\n",
    "\n",
    "Before we start setting up the tools, let's set up our brain. We can do this like before, using LangChain. \n",
    "\n",
    "üëâ Set up a model. This time use the Gemini 2.5 Flash model, not the Lite version because it tends not to work very well with our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà A first tool: check up on our favourite stocks\n",
    "\n",
    "Wouldn't it be great if we could ask our PA in natural language about our stocks?\n",
    "\n",
    "For this, our agent needs to be able to retrieve stock prices. Back in the data sourcing unit, we used the **Massive API** (formerly known as Polyon) to get stock prices. We spent a fair bit of time to use their API and source data from it.\n",
    "\n",
    "It turns out they have built a tool that we can plug straight into our agent. Under the hood it's using the same API, but it's completely ready for an agentic workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate with the Massive API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use, we'll have to authenticate though. Head to the [massive.com](https://massive.com/dashboard) website to get your API key. (Normally you should have already set up an account and an API key in the data sourcing unit. If not, create one. Note: Massive was previously known as Polygon, so you might have stored your password under that name.)\n",
    "\n",
    "1. Copy your API key from the website.\n",
    "1. Open `.env` file in this unit's folder (one level up from your challenge folder).\n",
    "1. Write a new line: `POLYGON_API_KEY=your_massive_api_key` in the file.\n",
    "1. Save and close the file.\n",
    "1. Run the cell below to reload the environment variables.\n",
    "\n",
    "> Why `POLYGON_API_KEY` if the API is called Massive? The LangChain tools haven't changed to the new name yet, and require an environment variable named `POLYGON_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "'POLYGON_API_KEY' in os.environ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Polygon tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that done, let's load the Polygon tools. \n",
    "\n",
    "> The LangChain tools are still named Polygon at the time of writing. If you can't find Polygon, try Massive instead.\n",
    "\n",
    "LangChain has quite a big [library of tools](https://docs.langchain.com/oss/python/integrations/tools) that we can use.\n",
    "\n",
    "Have a look at the library and find the Polygon tool documentation.\n",
    "\n",
    "üëâ Find in the documentation how you can load the tools. What is the output of the `.get_tools()` method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polygon gives us 4 tools.\n",
    "\n",
    "Throughout this challenge we'll create a `tools` list that will contain all the tools our agent can use.\n",
    "\n",
    "üëâ Create a list with the 4 Polygon tools to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an agent to use the tools\n",
    "\n",
    "Go and check the [LangChain quick start](https://docs.langchain.com/oss/python/langchain/quickstart) to see how you can create an agent. Just a basic agent for now.\n",
    "\n",
    "üëâ Create an agent and name it `agent_executor`. It should use the model and tools we created before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out.\n",
    "\n",
    "üëâ Ask the agent for a random stock's price. You can use the company name, it will probably figure out the ticker for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"Give me Apple's stock price?\"\n",
    "response = agent_executor.invoke(\n",
    "    {\"messages\": [HumanMessage(content=query)]}\n",
    ")\n",
    "response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that went wrong very quickly. What happened? Have a look at the end of the error message: it tells us that our free plan doesn't have the required accesses. It turns out our LLM was using the `polygon_last_quote` tool, which asks for current prices. We can only look at historical prices.\n",
    "\n",
    "We don't want our agent to completely crash for such a reason. So, let's add some error handling. Again, the [LangChain documentation](https://docs.langchain.com/oss/python/langchain/agents#tool-error-handling) shows us how to do this. It's not the core of what we want to do in this challenge, so here's the code you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import wrap_tool_call\n",
    "from langchain.messages import ToolMessage\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n",
    "    try:\n",
    "        return handler(request)\n",
    "    except Exception as e:\n",
    "        # Return a custom error message to the model\n",
    "        return ToolMessage(\n",
    "            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n",
    "            tool_call_id=request.tool_call[\"id\"],\n",
    "        )\n",
    "\n",
    "agent_executor = create_agent(model, tools, middleware=[handle_tool_errors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are doing here is adding some so called \"middleware\" that will catch errors from our tools, and relay that information to the model, instead of letting the whole thing crash.\n",
    "\n",
    "Let's use our improved model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Give me Apple's stock price?\"\n",
    "response = agent_executor.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much better, isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what our agent has been doing:\n",
    "\n",
    "While you're building your agent, it's better to not just show the last message, but also the intermediate steps.\n",
    "\n",
    "One way to do that is to look at the complete response instead, but that's not very user-friendly as you will see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, with this approach, we'd have to wait until the agent has finished all the steps before we see the result. If you have many tools and steps, that could take long.\n",
    "\n",
    "So let's put it in streaming mode to display the results as they are generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in agent_executor.stream({\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=query)\n",
    "    ]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's more interesting: now we can see the **Tool Message**: the output of the tool as it is handed over to the model by the error handler.\n",
    "\n",
    "And it tells us that our free plan doesn't have the required accesses. It turns out our LLM was using the `polygon_last_quote` tool, which asks for current prices. We can only look at historical prices.\n",
    "\n",
    "üëâ Try asking the agent for the **closing** price on a specific date (you can write the date like you want, just make sure it has day, month and year, and that it's was a working day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Give me Apple's closing price on the 5th of May 2025.\"\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's much better. The good thing is that we can also verify the intermediate steps: we can see which tool was used, which arguments were used, and what values it returned. This way we can verify that the LLM we use isn't hallucinating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Now ask the tool for \"yesterday's\" closing price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Give me yesterday's closing price of Apple.\"\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bummer, it doesn't know what date we are. That makes sense: after all, based on its training data, the LLM can't know what date we are. Let's solve that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÜ Adding the date\n",
    "\n",
    "To tell the LLM what date we are, we can give it a simple tool.\n",
    "\n",
    "üëâ Create your own custom tool `get_today`.\n",
    "\n",
    "Step-by-step instructions:\n",
    "- Start from the basic function below.\n",
    "- Convert it into a LangChain tool (like we did in the tool calling challenge).\n",
    "- Add a docstring to the function so the agent knows what the tool is supposed to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# TODO: Convert this function to a tool\n",
    "def get_today() -> str:\n",
    "    # TODO: Add a docstring\n",
    "    return datetime.today().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then add your `get_today` tool to the list of tools the agent can use. You'll have to reinstantiate your agent with the updated list. It doesn't update automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask your agent again for yesterday's closing price. You should see it calling the `get_today` function to figure out the date. Based on that it should then call the Polygon API for yesterday's price and return it. (Assuming that yesterday the stock exchange was open... In that case, ask it for the \"last working day's closing price\" instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Give me the last working day's closing price of Apple.\"\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù The memory problem, and how to solve it\n",
    "\n",
    "If you have been lucky, the agent so far always gave you a response without a follow up question.\n",
    "If it would have asked you for extra input, how would you be able to answer it? \n",
    "\n",
    "You could give it a new prompt. That assumes the agent still remembers the previous prompts and all of its answers. \n",
    "\n",
    "Does it? Let's try a simple experiment.\n",
    "\n",
    "üëâ Prompt the agent twice:\n",
    "1. Say hi, and tell it your name.\n",
    "1. Then ask it for your name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the agent\n",
    "response = agent_executor.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi, I'm Jules! and I live in Brussels.\")]}\n",
    ")\n",
    "response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's my name again?\")]}\n",
    ")\n",
    "response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer may vary, but it won't tell you your name. That's because your agent has no memory.\n",
    "\n",
    "To make it reuse earlier parts of our conversation, we'd have to feed it the whole message history when we prompt it with our next prompt. That's why `messages` in the `invoke` method is a list: the whole conversation history.\n",
    "\n",
    "So, we'd have to maintain a list with all the message, and each time append our new question and the new answer to it. That's very tedious. And we're only having one conversation now. In reality we'd have to manage multiple chats at the same time.\n",
    "\n",
    "Fortunately LangGraph helps us to do that with **memory**, or **checkpointers**.\n",
    "\n",
    "If your memory doesn't betray you, you already know what we'll tell you next:\n",
    "\n",
    "üëâ Go check the [LangGraph documentation](https://python.langchain.com/docs/tutorials/agents/) and add memory to your agent. Then try it with two separate prompts again. (Don't forget to add a `thread_id` in the `config`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "agent_executor = create_agent(model, tools, middleware=[handle_tool_errors], checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "response = agent_executor.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi, I'm Jules! and I live in Brussels.\")]},\n",
    "    config=config\n",
    ")\n",
    "response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "response = agent_executor.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's my name again?\")]},\n",
    "    config=config,\n",
    ")\n",
    "response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are also able to answer the agent's follow-up questions.\n",
    "\n",
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Give me Apple's closing price.\"\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]},\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Last Friday's\"\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]},\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"And Amazon's?\"\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]},\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That starts to look like something.\n",
    "\n",
    "By combining tools to get stock prices and today's date with memory, the agent starts to work pretty well.\n",
    "\n",
    "But we've been careful in our prompts: we always asked it for the closing price. If we had asked for the current price, it would still fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Give me Meta's current price.\"\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]},\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we solve that?\n",
    "\n",
    "We can give the agent a bit more guidance. It should not use the current price API endpoint if it's not authorized too, but fall back to the historical prices (i.e. the last closing price).\n",
    "\n",
    "We are using a pre-built tool, so we can't change the tool itself. But we can do someting else..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóíÔ∏è Adding a system prompt\n",
    "\n",
    "So far we didn't provide any system prompt to our agent. And in case you wondered: no, LangGraph hasn't added a default one for us either.\n",
    "\n",
    "We could go and search for an existing one in the [LangChain Hub](https://smith.langchain.com/hub), but let's build one ourselves for our specific use case.\n",
    "\n",
    "üëâ Write a `system_prompt` and add it to your model. The prompt should instruct the agent to fall back on historical prices if the API doesn't allow to get current prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your agent with the new prompt (notice how we changed the `thread_id` to start a new conversation - just to be sure the agent isn't influenced by our previous question):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Give me Meta's current price.\"\n",
    "config = {\"configurable\": {\"thread_id\": \"abc126\"}}\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]},\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ Congratulations! You know have a working agent.\n",
    "\n",
    "Let's look back at what we did to get this working:\n",
    "\n",
    "üß† We instantiated an LLM. The brain of our agent.\n",
    "\n",
    "üß∞ We created a toolbox for our agent, with:\n",
    "\n",
    "   - üìà The Polygon tool to get stock prices\n",
    "\n",
    "   - üóìÔ∏è A simple custom made tool to get today's date\n",
    "\n",
    "üìù We added memory to our agent, so we could have an interactive conversation with it.\n",
    "\n",
    "üóíÔ∏è We added a system prompt to give it more guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè≠ Putting your agent in production\n",
    "\n",
    "You probably already noticed that using the agent like this in the notebook is a bit clumsy.\n",
    "\n",
    "In this part, we'll turn our agent into a real application. We'll make a CLI tool that you can interact with in the terminal (CLI = Command Line Interface).\n",
    "\n",
    "Open `my_pa.py` and investigate it. We have added some basic functionality to it. Up to you to make it run the agent we created in this notebook.\n",
    "\n",
    "When you run the file through `python my_pa.py`, the `main()` function will start an (almost infinite) loop. At each iteration it:\n",
    "- Asks the user for a new prompt\n",
    "- Uses the agent with the new prompt\n",
    "- Outputs the agent's response\n",
    "- And starts all over\n",
    "\n",
    "üëâ Your task is to fill in the gaps:\n",
    "- Add the necessary imports\n",
    "- Instantiate the tools (ignore the Wikipedia and recipe tools for now, we'll add them later)\n",
    "- Instantiate the LLM and the agent\n",
    "- Complete the `use_agent()` function \n",
    "\n",
    "Everything is already in the notebook. It's mainly a question of copying the good bits into the `.py` file and make it all work together.\n",
    "\n",
    "Once you're done with that, come back to this notebook. We'll add a couple more tools to our agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üç∞ Adding our own recipes tool\n",
    "\n",
    "In the tool calling challenge, we created a tool to retrieve recipes. Let's add that to our agent.\n",
    "\n",
    "We included the tool in `recipe.py`. Have a look at it. Notice that we included the `@tool` directly in here.\n",
    "\n",
    "We just need to import it, and check that it's a tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recipe import get_recipes\n",
    "type(get_recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Give your agent access to this new tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Give me recipes with chicken.\")]},\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the agent\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Give me the first recipe's instructions.\")]},\n",
    "    config=config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That last one is not very useful. It would be handy if we'd actually get the recipe itself, no?\n",
    "\n",
    "We can do that with the `RequestsGetTool`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.requests.tool import RequestsGetTool\n",
    "from langchain_community.utilities.requests import TextRequestsWrapper\n",
    "\n",
    "requests_tool = RequestsGetTool(\n",
    "    requests_wrapper=TextRequestsWrapper(headers={}), allow_dangerous_requests=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöß See that `allow_dangerous_requests` argument? We need to set it explicitly to make our requests work.\n",
    "\n",
    "‚ö†Ô∏è It also means it's **dangerous**: our agent can now make any kind of GET request it wants. This could do unintended things, and could be abused by end users of your agent. If you are deploying your agent to let other people use it, you definitely do not want to include this.\n",
    "\n",
    "üëâ We're only making a Personal Agent for ourselves, so let's add it to the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the new model, and ask it for the recipe. You might have to nudge it a bit in the good direction with your prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our agent now can fetch information from our recipes site, but it can also make requests to other sites or APIs. That's why it's dangerous. Ask it to get information from your favourite news site.\n",
    "\n",
    "Sometimes it will be able to extract the information from the sites, sometimes it will only be able to give you the pure HTML. There are better [tools](https://python.langchain.com/docs/integrations/tools/#web-browsing) than Requests for that, but you'd have to sign up for them. That's why we stuck to a very basic tool for this demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Extending the agent's knowledge base\n",
    "\n",
    "The LLM only knows what it learned during training. But we could easily extend its knowledge with the Wikipedia tool.\n",
    "\n",
    "Add [Wikipedia](https://python.langchain.com/docs/integrations/tools/wikipedia/) to your agent's tool box, and try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your agent and ask it about something that happened recently, and see what tool it uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè≠ Add the new tools to `my_pa.py`\n",
    "\n",
    "After experimenting in the notebook with new tools, bring them over to `my_pa.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Congratulations! You made a fully working agent.\n",
    "\n",
    "You learned how to:\n",
    "\n",
    "ü¶æ Create an agent , starting from an LLM üß†\n",
    "\n",
    "üìù Add memory to the agent to allow interactive conversations\n",
    "\n",
    "üß∞ Give it diffent tools, both existing ones and self-built custom tools\n",
    "\n",
    "üóíÔ∏è Tweak its behaviour using a system prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Want to take it further?\n",
    "\n",
    "Here are some ideas you can work with:\n",
    "\n",
    "- Can you make the agent to always respond in your own language?\n",
    "- Make it a bit more easy to work with the recipes?\n",
    "- What other tools could you add for your daily tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to commit and push all your work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
